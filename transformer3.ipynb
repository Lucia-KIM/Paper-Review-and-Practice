{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer3",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNEOsmgd2JLZqwhC/bkSF+J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lucia-KIM/Paper-Review-and-Practice/blob/main/transformer3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "xflLnitysVc-",
        "outputId": "9b222863-095c-43c1-dfc0-d4ce800a4155"
      },
      "source": [
        "import torch\r\n",
        "\r\n",
        "torch.__version__"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.7.1+cu101'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7eOE-wUoDET"
      },
      "source": [
        "## 트랜스포머 아키텍쳐 구현\n",
        "\n",
        "### Multi Head Attention \n",
        "\n",
        "- Scaled Dot-Product Attention 구현해서, \n",
        "- Multi-Head Attention으로\n",
        "- Scaled Dot-Product attention을 겹쳐놓은 h개의 Scaled Dot-Product attention(Multi-Head attention)을 사용\n",
        "- 각각의 Attention에는 Query, Key, Value가 사용되었는데, 이 세 값들을 바로 사용하는 것이 아니라 h개의 Attention각각에 대해서 다르게 초기화된 Parameter Matrix를 곱하여 (Projection) 사용한다. \n",
        "- 즉, 이 과정에서 h개의 다른 값들을 얻게되고 이를 concat해서 attention결과물을 얻기 때문에 하나의 Scaled Dot-Product Attention을 사용할 때보다 주어진 Query, Key, Value에 대해서 보다 다양한 상황(subspace)에 대한 attention을 계산할 수 있게 된다. \n",
        "\n",
        "#### 1) Multi-Head Attention은 쿼리, 키, 벨류가 Linear로 각각 scaled dot-product attention에 들어가서 concat되는 형식임 \n",
        "#### 2) Scaled Dot-Product Attention은 Q와 K를 행렬 곱하고, 스케일 한 다음에 마스크를 씌우고(선택) 소프트 맥스를 거친 결과를 V와 행렬곱한다. \n",
        "- Query가 들어오게 되면 Key값과의 계산을 통하여 기준 값 Key에 대한 Query의 상대적인 weight(softmax)를 계산한다.\n",
        "- 이 Weight를 Value에 곱함으로써 Query와 Key의 연관성을 기준으로 한 새로운 값을 얻게 된다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpTbvBHmFJk7"
      },
      "source": [
        "import torch.nn as nn\n",
        "# TORCH.NN: 신경망(neural network) 구조가 디자인 된 모듈과 클래스들을 제공, 필요에 따라  커스터마이즈하여 사용\n",
        "# 하이퍼 파라미터로 hidden_dim, n_heads, dropout_ratio 지정 \n",
        "\n",
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "    def __init__(self, hidden_dim, n_heads, dropout_ratio, device): \n",
        "        super().__init__()   # 생성자 \n",
        "\n",
        "        assert hidden_dim % n_heads == 0  \n",
        "        # assert는 뒤의 조건이 True가 아니면 AssertError를 발생\n",
        "        # 나머지가 0인 경우를 찾는다. \n",
        "\n",
        "        self.hidden_dim = hidden_dim  # 하나의 단어에 대한 임베딩 차원\n",
        "        self.n_heads = n_heads # head의 개수 = scaled dot-product attention의 개수\n",
        "        self.head_dim = hidden_dim // n_heads # 임베딩 차원을 헤드 개수로 나눈 몫, 즉 각 헤드에서의 임베딩 차원\n",
        "\n",
        "        self.fc_q = nn.Linear(hidden_dim, hidden_dim) # Query 값에 적용될 fc레이어\n",
        "        self.fc_k = nn.Linear(hidden_dim, hidden_dim) # Key값에 적용될 fc레이어\n",
        "        self.fc_v = nn.Linear(hidden_dim, hidden_dim) # Value 값에 적용될 fc레이어\n",
        "\n",
        "        self.fc_o = nn.Linear(hidden_dim, hidden_dim) # output linear\n",
        "        self.dropout = nn.Dropout(dropout_ratio) # 드롭 아웃 비율\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device) \n",
        "        # 이후 스케일로 나눈 값에 소프트맥스를 씌워서 사용할 예정 \n",
        "\n",
        "    def forward(self, query, key, value, mask = None):\n",
        "        # mask = None인 경우\n",
        "\n",
        "        batch_size = query.shape[0] # 쿼리 행렬의 행의 개수\n",
        "        # query의 모양은 [batch_size, query_len, hidden_dim] 형태, 여기서 인덱스 0번 batch_size을 받음\n",
        "\n",
        "        Q = self.fc_q(query)\n",
        "        K = self.fc_k(key)\n",
        "        V = self.fc_v(value)\n",
        "\n",
        "        # hidden_dim -> n_heads * head_dim 형태로 변경\n",
        "        # --> 하나의 단어에 대한 임베딩 차원을 각 헤드의 임베딩 차원*어텐션의 갯수로 변경\n",
        "        # n_heads(h)개의 서로 다른 어텐션 컨셉을 학습하게 함\n",
        "\n",
        "        # view()는 tensor의 모양을 바꾸는데 사용(데이터의 구조가 변경될 뿐 순서는 변경되지 않는다)\n",
        "        # view가 반환한 tensor는 원본 tensor와 기반이 되는 data를 공유한다. 만약 반환된 tensor의 값이 변경된다면, viewed되는 tensor에서 해당하는 값이 변경된다.\n",
        "        # permute(순서 인덱스)는 모든 차원들을 맞교환할 수 있다.\n",
        "        # 지금까지 Q는 [batch_size, query_len, hidden_dim] 형태로 저장되었음. \n",
        "        # 이걸 view()를 사용해 Q: [batch_size, n_heads, query_len, head_dim]로 텐서의 모양을 바꿔줌 \n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0,2,1,3)\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0,2,1,3)\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0,2,1,3)\n",
        "\n",
        "        \n",
        "        # (Attention Energy) Scaled Dot-Product Attention을 만들기 위해, \n",
        "        # input은 d_k dimension의 queries와 keys, 그리고 d_v dimension의 values로 구성된다. \n",
        "        # query와 key의 dot product를 계산하여 \\sqrt{d_k}로 나눈다.\n",
        "        # \\sqrt{d_k}로 나누어 준, 다시말해 scaled하였기 때문에 Scaled Dot-Product Attention\n",
        "        # 이후 softmax function**을 사용하여 values에 대한 weights를 얻어 낸다.\n",
        "        energy = torch.matmul(Q, K.permute(0,1,3,2))/self.scale\n",
        "        # matmul(): 3차원 이상의 행렬끼리 곱\n",
        "        # energy: [batch_size, n_heads, query_len, key_len]\n",
        "        # 여기서 Q는 [batch_size, n_heads, query_len, head_dim],\n",
        "        # K.permute(0,1,3,2)는 [batch_size, n_heads, head_dim, key_len]\n",
        "        # 그럼 head_dim* query_len = query_len이고, head_dim*key_len = key_len인가? \n",
        "\n",
        "\n",
        "        # 마스크를 사용하는 경우\n",
        "        if mask is not None:\n",
        "            # 마스크를 부분을 아주 작은 값으로 채우기 -1e10\n",
        "            energy = energy.masked_fill(mask==0, -1e10)\n",
        "\n",
        "        # 소프트맥스로 어텐션 스코어 계산 : 각 단어에 대한 확률\n",
        "        attention = torch.softmax(energy, dim=-1)\n",
        "        # query와 key에 대한 dot-product를 계산하면 각각의 query와 key 사이의 유사도를 구할 수 있게 된다.\n",
        "        # attention: [batch_size, n_heads, query_len, key_len]\n",
        "\n",
        "        # 여기에서 Scaled Dot-Product Attention을 계산\n",
        "        x = torch.matmul(self.dropout(attention), V)\n",
        "        # key와 value는 attention이 이루어지는 위치에 상관없이 같은 값을 갖게 되는데, \n",
        "        # 여기서 x는 [batch_size, n_heads, query_len, head_dim] 이다.\n",
        "\n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        # x: [batch_size, query_len, n_heads, head_dim]\n",
        "        # contiguous()는 새로운 메모리에 할당하여 주소값 재배열이 가능하다. \n",
        "        \n",
        "        x = x.view(batch_size, -1, self.hidden_dim)\n",
        "        #self.head_dim = hidden_dim // n_heads 였으니, head_dim*n_heads = hidden_dim\n",
        "        # 그래서 x는 [batch_size, query_len, hidden_dim]\n",
        "\n",
        "        x = self.fc_o(x)\n",
        "        # x: [batch_size, query_len, hidden_dim]\n",
        "\n",
        "        return x, attention"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jX3DNb49VAm0"
      },
      "source": [
        "## Position-wise Feedforward Networks\n",
        "- 인코더와 디코더의 각각 position(개별 단어마다)에 개별적으로 동일하게 적용되는 fully connected feed-forward network sub-layerrk 있음 \n",
        "- ReLU 함수를 포함한 두개의 선형 변환(linear transformation 1, 2)으로 구성됨\n",
        "- input(x) -> linear transformation_1 -> ReLU -> linear transformation_2 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJQQSwZYVAO8"
      },
      "source": [
        "class PositionwiseFeedforwardLayer(nn.Module):\n",
        "    def __init__(self, hidden_dim, pf_dim, dropout_ratio):\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc_1 = nn.Linear(hidden_dim, pf_dim) # linear transformation_1\n",
        "        self.fc_2 = nn.Linear(pf_dim, hidden_dim) # linear transformation_2\n",
        "        # hidden_dim: 하나의 단어에 대한 임베딩 차원\n",
        "        # pf_dim: Feedforward 레이어에서의 내부 임베딩 차원\n",
        "        # hidden_dim 만큼의 차원이 들어와서 최종 hidden_dim 만큼의 차원을 내보냄\n",
        "        # 다시말해, 입력과 출력의 차원이 동일함\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_ratio) # dropout_ratio: 드롭아웃(dropout) 비율\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
        "        # 선형변화 1에 ReLU를 적용한 x\n",
        "        # x: [batch_size, seq_len, hidden_dim]\n",
        "\n",
        "        x = self.fc_2(x)\n",
        "        # 이어서 선형변화 2를 적용함 \n",
        "\n",
        "        return x"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgsVwR7lacaK"
      },
      "source": [
        "## Encoder Layer\n",
        "- 하나의 인코더 레이어를 구현\n",
        "- 입력과 출력의 차원이 같다.\n",
        "- 트랜스포머는 이런 인코더 레이어를 여러번 중첩하여 사용한다. \n",
        "\n",
        "### 간단한 구조는, \n",
        "    1) 소스에 대한 어텐션 + 정규화\n",
        "    2) positionwise feedforward + 정규화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcIRiSkkbPOJ"
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, hidden_dim, n_heads, pf_dim, dropout_ratio, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hidden_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hidden_dim)\n",
        "        # Layer Normalization\n",
        "        # 정규화란 무언가를 표준화 시키거나 다른 것과 비교하기 쉽도록 바꾸는 것\n",
        "        # 따라서, 정규화는 데이터의 범주를 바꾸는 작업으로 스케일이 곧 정규화\n",
        "        #선형대수학에서 놈은 벡터의 크기(magnitude) 또는 길이(length)를 측정하는 방법을 의미\n",
        "\n",
        "        self.self_attention = MultiHeadAttentionLayer(hidden_dim, n_heads, dropout_ratio, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hidden_dim, pf_dim, dropout_ratio)\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "    # 하나의 임베딩이 복제되어 쿼리, 키, 벨류로 입력되는 방식\n",
        "    def forward(self, src, src_mask):\n",
        "        # src(소스는) [batch_size, src_len, hidden_dim]\n",
        "        # src_mask : [batch_size, src_len]\n",
        "\n",
        "        # 인코더 self-attention layer\n",
        "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
        "        # 필요한 경우 self attention에서 마크스 행렬을 이용하여 어텐션할 단어를 조절 가능\n",
        "        # src-src 이거나, src-src_masked\n",
        "        # 디코더에서는 인코더와 달리 순차적으로 결과를 만들어야 하기 때문에 mask를 사용함\n",
        "        # 다시말해 특정 단어 position i보다 뒤에 있는 position에 어텐션을 주지 못하게 함\n",
        "\n",
        "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
        "        # dropout, residual connection and layer norm\n",
        "        # 각각의 계산을 진행할 때는 Residual Connection{=LayerNorm(x + Sublayer(x))}과 Layer normalization을 적용한다.\n",
        "        # Residual Connection의 개념은 기존에 학습한 정보를 보존하고, 거기에 추가적으로 학습한 정보를 더하는 형식이다.\n",
        "        # x의 아웃풋이 y라고 할때, y=f(x)는 direct로 학습한 경우라고 생각할 수 있다. 아웃풋인 y는 x를 통해 새롭게 학습한 정보를 의미한다.\n",
        "        # y=f(x)인 경우는 기존에 학습한 정보를 보존하지 않고 변형시켜 새롭게 생성하는 정보이다.\n",
        "        # 이 경우 레이어의 깊이 깊어질수록 한번에 학습해야 할 mapping이 너무 많아져 학습이 어려워지는 문제가 발생한다.\n",
        "        # 반면에 Residual Connection은 y=f(x)+x로 이전에 학습한 내용 x를 더해주어(보존)하여 추가되는 학습만 진행하면 된다는 장점이 있다.\n",
        "        # 논문에서는 Residual Connection이 포함된 self attention의 장점으로 layer당 계산량이 줄어든다고 기재해 놓았다. \n",
        "        \n",
        "        # position-wise feedforward\n",
        "        _src = self.positionwise_feedforward(src)\n",
        "\n",
        "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
        "        # dropout, residual and layer norm\n",
        "        # src: [batch_size, src_len, hidden_dim]\n",
        "        \n",
        "        return src"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTPk4qJKRbv6"
      },
      "source": [
        "## 인코더 아키텍처\r\n",
        "\r\n",
        ": 전체 인코더 아키텍처 정의\r\n",
        "\r\n",
        "- 전처리 후 토큰화 된 소스를 input embedding\r\n",
        "- positional encoding을 거쳐 encoder 박스로 들어감.\r\n",
        "    * 트랜스포머는 단어의 순서(sequence)를 이용하기 위해 position에 대한 정보를 추가해 주어야 함.\r\n",
        "    * 논문에서는 서로 다른 빈도의 사인과 코사인 함수를 이용한다. \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vt4hLjmuRYdQ"
      },
      "source": [
        "class Encoder(nn.Module):\r\n",
        "    def __init__(self, input_dim, hidden_dim, n_layers, n_heads, pf_dim, dropout_ratio, device, max_length=100):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        self.device = device\r\n",
        "        \r\n",
        "        # nn.Embedding()는 임베딩 층(embedding layer)을 만들어 훈련 데이터로부터 처음부터 임베딩 벡터를 학습하는 방법\r\n",
        "        # nn.Embedding()을 사용하여 학습가능한 임베딩 테이블 만든다. \r\n",
        "        # nn.Embedding은 크게 두 가지 인자를 받는데, \r\n",
        "        # 1) num_embeddings : 임베딩을 할 단어들의 개수. 다시 말해 단어 집합의 크기\r\n",
        "        # 2) embedding_dim : 임베딩 할 벡터의 차원\r\n",
        "        self.tok_embedding = nn.Embedding(input_dim, hidden_dim)\r\n",
        "        # input_dim: 하나의 단어에 대한 원 핫 인코딩 차원\r\n",
        "        # hidden_dim: 하나의 단어에 대한 임베딩 차원\r\n",
        "\r\n",
        "        # 인풋 차원으로 들어온 것을 임베딩 차원으로 바꿔주고, \r\n",
        "        # 위치에 대한 정보 값을 더해주기 위해 pos 임베딩 레이어를 추가한다. \r\n",
        "\r\n",
        "        self.pos_embedding = nn.Embedding(max_length, hidden_dim)\r\n",
        "        # max_length: 문장 내 최대 단어 개수\r\n",
        "\r\n",
        "        self.layers = nn.ModuleList([EncoderLayer(hidden_dim, n_heads, pf_dim, dropout_ratio, device) for _ in range(n_layers)])\r\n",
        "        # layer의 갯수는 앞서 만들어 놓은 인코더 레이어의 수를 가져온다.\r\n",
        "        # nn.ModuleList()는 각 레이어를 리스트에 전달하고 레이어의 iterator(반복)를 만든다.\r\n",
        "        # 인코더 레이어의 수 만큼 for를 돌려서 layer의 수를 정한다.\r\n",
        "\r\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\r\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hidden_dim])).to(device)\r\n",
        "\r\n",
        "    def forward(self, src, src_mask):\r\n",
        "\r\n",
        "        batch_size = src.shape[0]  # 문장의 개수\r\n",
        "        src_len = src.shape[1] # 각 문장들 중 단어의 개수가 가장 많은 문장의 단어 개수\r\n",
        "        # src는 [batch_size, src_len]로 구성되어 있음\r\n",
        "\r\n",
        "        # positional encoder를 논문과 다르게 학습하는 형태로 구현\r\n",
        "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\r\n",
        "        # unsqueeze함수는 지정한 자리에 1인 차원을 생성하는 함수이다. \r\n",
        "        # repeat(object, n)는 object를 n번 반복함 - 각각의 문장마다 수행하도록 repeat함\r\n",
        "        # 여기서는 소스의 길이만큼 1차원 텐서를 만들어서, unsqueeze()를 통해 2차원으로 만들고,\r\n",
        "        # unsqueeze로 만든 1차원 자리에 배치 사이즈만큼의 차원으로 바꿔준다. \r\n",
        "        # 결국 pos는 [batch_size, src_len] 형태가 된다.\r\n",
        "\r\n",
        "        # 소스 문장의 임베딩과 위치 임베딩을 더한 것을 실제 입력 값으로 사용\r\n",
        "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\r\n",
        "\r\n",
        "        # 모든 인코더 레이어를 차례대로 거치면서 순전파(forward) 수행\r\n",
        "        for layer in self.layers:\r\n",
        "            src = layer(src, src_mask)\r\n",
        "        # src: [batch_size, src_len, hidden_dim]\r\n",
        "\r\n",
        "        return src # 마지막 레이어의 출력을 반환"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1b1m-fRgc11"
      },
      "source": [
        "## Decoder Layer \r\n",
        ": 하나의 디코더 레이어 정의\r\n",
        "- 트랜스포머의 디코더는 디코더 레이어를 여러 번 중첩해 사용\r\n",
        "- 디코더 레이어에서는 2개의 Multi-Head Attention 레이어가 사용 됨\r\n",
        "    * 타겟 문장에서 각 단어는 다음 단어가 무엇인지 알 수 없도록 만들기 위해 Masked Multi-Head Attention 사용\r\n",
        "    * 인코더에서 출력된 내용에 대한 Multi-Head Attention\r\n",
        "- 이후에 positionwise feedforward를 진행함\r\n",
        "\r\n",
        "### 전체 구조를 간단히 보면,\r\n",
        "    1) 타겟에 대한 마스크 어텐션 + 정규화\r\n",
        "    2) 인코더 출력에 대한 어텐션 + 정규화\r\n",
        "    3) positionwise feedforward + 정규화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8K0lnXzc6oh"
      },
      "source": [
        "class DecoderLayer(nn.Module):\r\n",
        "    def __init__(self, hidden_dim, n_heads, pf_dim, dropout_ratio, device):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hidden_dim)\r\n",
        "        self.enc_attn_layer_norm = nn.LayerNorm(hidden_dim)\r\n",
        "        self.ff_layer_norm = nn.LayerNorm(hidden_dim)\r\n",
        "        self.self_attention = MultiHeadAttentionLayer(hidden_dim, n_heads, dropout_ratio, device)\r\n",
        "        self.encoder_attention = MultiHeadAttentionLayer(hidden_dim, n_heads, dropout_ratio, device)\r\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hidden_dim, pf_dim, dropout_ratio)\r\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\r\n",
        "\r\n",
        "    # 인코더의 출력 값(enc_src)을 어텐션(attention)하는 구조 \r\n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\r\n",
        "        # trg: [batch_size, trg_len, hidden_dim]\r\n",
        "        # enc_src: [batch_size, src_len, hidden_dim]\r\n",
        "        # trg_mask: [batch_size, trg_len]\r\n",
        "        # src_mask: [batch_size, src_len]\r\n",
        "\r\n",
        "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\r\n",
        "        # 타겟 데이터 스스로 자신에게 어텐션하는 구조\r\n",
        "        # 쿼리, 키, 벨류 모두 자기 자신을 넣을 수 있게 trg로 \r\n",
        "\r\n",
        "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\r\n",
        "        # dropout, residual connection and layer norm\r\n",
        "\r\n",
        "        # 디코더의 쿼리(Query)를 이용해 인코더를 어텐션(attention)\r\n",
        "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\r\n",
        "        # encoder_attention은 x(단어)와 attention(확률)을 반환 \r\n",
        "        # 쿼리는 디코더에 포함되어 있는 출력 단어들에 대한 정보 (trg)\r\n",
        "        # 인코더에서 가장 마지막으로 출력된 값(enc_src)을 키로 사용한다.\r\n",
        "        \r\n",
        "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\r\n",
        "        # encoder에서 받은 x(_trg)와 trg로 encoder attention layer의 normalization 한다.\r\n",
        "        # dropout, residual connection and layer norm\r\n",
        "        # trg: [batch_size, trg_len, hidden_dim]\r\n",
        "\r\n",
        "        # 타겟을 masked 어텐션한 것 1층\r\n",
        "        # 인코더의 출력 내용 어텐션 1층\r\n",
        "        # 이제 positionwise feedforward 할 차례임\r\n",
        "        _trg = self.positionwise_feedforward(trg)\r\n",
        "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\r\n",
        "\r\n",
        "        return trg, attention\r\n",
        "        # trg: [batch_size, trg_len, hidden_dim]\r\n",
        "        # attention: [batch_size, n_heads, trg_len, src_len]"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5lkJ2_atszk"
      },
      "source": [
        "## 디코더 아키텍처\r\n",
        ": 전체 디코더 아키텍처 정의\r\n",
        "- 이번에도 논문과 다르게, 위치 임베딩(positional embedding)을 학습하는 형태로 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTXEdCpTc-3H"
      },
      "source": [
        "class Decoder(nn.Module):\r\n",
        "    def __init__(self, output_dim, hidden_dim, n_layers, n_heads, pf_dim, dropout_ratio, device, max_length=100):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        self.device = device\r\n",
        "        self.tok_embedding = nn.Embedding(output_dim, hidden_dim)\r\n",
        "        # output_dim: 하나의 단어에 대한 원 핫 인코딩 차원\r\n",
        "        # hidden_dim: 하나의 단어에 대한 임베딩 차원\r\n",
        "        # 단어의 개수와 같은 차원을 임베딩 차원으로 바꿔주고, \r\n",
        "        \r\n",
        "        self.pos_embedding = nn.Embedding(max_length, hidden_dim)\r\n",
        "        # 위치에 대한 정보를 주기위해 전체 시퀀스의 길에 해당하는 차원을 hidden_dim으로 바꿔준다.\r\n",
        "\r\n",
        "        self.layers = nn.ModuleList([DecoderLayer(hidden_dim, n_heads, pf_dim, dropout_ratio, device) for _ in range(n_layers)])\r\n",
        "        self.fc_out = nn.Linear(hidden_dim, output_dim)\r\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\r\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hidden_dim])).to(device)\r\n",
        "\r\n",
        "    # 인코더의 마지막 레이어에서 나온 출력값과 타켓문장에 대한 정보를 받는다.\r\n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\r\n",
        "        batch_size = trg.shape[0]\r\n",
        "        trg_len = trg.shape[1]\r\n",
        "        # trg: [batch_size, trg_len]\r\n",
        "        \r\n",
        "        # 출력 문장도 0부터 단어의 개수에 대한 위치 정보를 담기 위해 초기화한 텐서를 생성하여 각 문장에 대해 동일하게 적용할 수 있게 만든다. \r\n",
        "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\r\n",
        "        # pos: [batch_size, trg_len]\r\n",
        "\r\n",
        "        # 문장의 임베딩 값에 위치에 대한 정보를 더한 값을 사용\r\n",
        "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\r\n",
        "        # 토큰 임베딩을 스케일해서, positional 임베딩과 더함. \r\n",
        "        # 이걸 드롭아웃 처리\r\n",
        "        # trg에 hidden_dim이 더해져서 [batch_size, trg_len, hidden_dim] 구조가 됨\r\n",
        "\r\n",
        "        # self.layers는 nn.ModuleList()로 decoder layers의 레이어를 하나씩 담은 리스트\r\n",
        "        for layer in self.layers:\r\n",
        "            # 소스 마스크와 타겟 마스크 모두 사용\r\n",
        "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)            \r\n",
        "        # trg: [batch_size, trg_len, hidden_dim]\r\n",
        "        # attention: [batch_size, n_heads, trg_len, src_len]\r\n",
        "\r\n",
        "        # 디코더 레이어를 여러번 거쳐 마지막에 나온 trg 값에 출력을 위한 linear layer을 거치게 만든다.\r\n",
        "        output = self.fc_out(trg) # fc_out은 output을 위한 linear layer\r\n",
        "        # self.fc_out가 nn.Linear(hidden_dim, output_dim)이니\r\n",
        "        # 최종 output은 [batch_size, trg_len, output_dim] 형태\r\n",
        "        \r\n",
        "        return output, attention"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIOUd26M5d2B"
      },
      "source": [
        "## 트랜스포머(Transformer) 아키텍처\r\n",
        ": 최종 전체 트랜스포머 모델 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ps4VldHdKxV"
      },
      "source": [
        "class Transformer(nn.Module):\r\n",
        "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        self.encoder = encoder\r\n",
        "        self.decoder = decoder\r\n",
        "        self.src_pad_idx = src_pad_idx\r\n",
        "        self.trg_pad_idx = trg_pad_idx\r\n",
        "        self.device = device\r\n",
        "        # 소스 문장의 <pad> 토큰(padding)에 대하여 마스크(mask) 값을 0으로 설정\r\n",
        "\r\n",
        "    def make_src_mask(self, src):\r\n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\r\n",
        "        # src는 [batch_size, src_len]의 형태, 패딩 토큰을 제외하고 \r\n",
        "        # 일반 소스에는 unsqueeze() 함수로 [batch_size, 1, 1, src_len] 구조로 바꿔준다.\r\n",
        "        # src_mask: [batch_size, 1, 1, src_len]\r\n",
        "\r\n",
        "        return src_mask\r\n",
        "\r\n",
        "    # 타겟 문장에서는 다음 단어가 무엇인지 모르게 mask사용\r\n",
        "    def make_trg_mask(self, trg):\r\n",
        "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\r\n",
        "        # trg: [batch_size, trg_len]\r\n",
        "        # trg_pad_mask: [batch_size, 1, 1, trg_len]\r\n",
        "\r\n",
        "        trg_len = trg.shape[1]\r\n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\r\n",
        "        # tril()은 행렬의 대각행렬 아래쪽을 지정한 수로 바꾸고 위쪽은 0으로 반환\r\n",
        "        # 여기서는 ones()를 통해 1로 바꿈, 아래와 같은 형식\r\n",
        "        \"\"\" (마스크 예시)\r\n",
        "        1 0 0 0 0\r\n",
        "        1 1 0 0 0\r\n",
        "        1 1 1 0 0\r\n",
        "        1 1 1 1 0\r\n",
        "        1 1 1 1 1\r\n",
        "        \"\"\"\r\n",
        "        # trg_sub_mask: [trg_len, trg_len]\r\n",
        "\r\n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\r\n",
        "        # &는 비트연산자로 AND 연산한다. 둘다 참일때만 만족\r\n",
        "        # 여기서는 Element wise로 and 연산을 수행한다. 즉, 각 행렬의 원소끼리만 곱한 것을 의미한다.\r\n",
        "        # 결과적으로는 두 마스크에서 값이 둘다 1인 경우에만 어텐션 스코어를 만들 수 있도록 만든다.\r\n",
        "\r\n",
        "        # trg_mask: [batch_size, 1, trg_len, trg_len]\r\n",
        "        return trg_mask\r\n",
        "\r\n",
        "    def forward(self, src, trg):\r\n",
        "        src_mask = self.make_src_mask(src)\r\n",
        "        trg_mask = self.make_trg_mask(trg)\r\n",
        "        # src: [batch_size, src_len]\r\n",
        "        # trg: [batch_size, trg_len]\r\n",
        "        # src_mask: [batch_size, 1, 1, src_len]\r\n",
        "        # trg_mask: [batch_size, 1, trg_len, trg_len]\r\n",
        "\r\n",
        "        # 인코더에 소스 문장을 넣어 인코더 출력값을 생성\r\n",
        "        enc_src = self.encoder(src, src_mask)\r\n",
        "        # enc_src: [batch_size, src_len, hidden_dim]\r\n",
        "\r\n",
        "        # 디코더는 인코더의 출력값(enc_src)에 매번 어텐션할 수 있도록 만든다.\r\n",
        "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\r\n",
        "        # output: [batch_size, trg_len, output_dim]\r\n",
        "        # attention: [batch_size, n_heads, trg_len, src_len]\r\n",
        "\r\n",
        "        return output, attention\r\n",
        "        # 최종 출력된 output값이 번역 결과가 된다. "
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inXnxQ48E-GF"
      },
      "source": [
        ""
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IY3-Hsg-HCz_"
      },
      "source": [
        "## 모델 학습하기 \r\n",
        "\r\n",
        "### 작업 순서 \r\n",
        "    1) 학습할 데이터 불러오기\r\n",
        "    2) 데이터 전처리\r\n",
        "    3) 모델 초기화 및 파라미터 설정\r\n",
        "    4) 모델 학습 및 검증 (loss 계산)\r\n",
        "    5) 학습동안 베스트 파라미터 도출 \r\n",
        "    6) 학습된 모델 저장  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MA6RULDp3VA-",
        "outputId": "caca6cb1-39ad-4c99-8be0-88150a11b2bb"
      },
      "source": [
        "!pip install torchtext==0.6.0"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchtext==0.6.0 in /usr/local/lib/python3.7/dist-packages (0.6.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.7.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (4.41.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (0.1.95)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchtext==0.6.0) (3.7.4.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (2020.12.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3tAKg__HmEI"
      },
      "source": [
        "## 데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmBezBvwBXUk"
      },
      "source": [
        "%%capture   \r\n",
        "#출력 내용이 나오지 않도록 억제\r\n",
        "\r\n",
        "# 토큰화를 지원하는 spacy라이브러리를 사용할 예정 \r\n",
        "!python -m spacy download en\r\n",
        "!python -m spacy download de\r\n",
        "\r\n",
        "# ! 는 작업의 강제성을 부여, 관리자 권한을 줄때(sudo처럼)\r\n"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bN0hUcVhHsLG"
      },
      "source": [
        "import spacy\r\n",
        "spacy_en = spacy.load('en')  # 영어 토큰화 기법이 담긴 객체 생성\r\n",
        "spacy_de = spacy.load('de') # 독일어 토큰화"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUDBFBJXHuxJ"
      },
      "source": [
        "# 각 언어의 토큰화 함수 정의\r\n",
        "\r\n",
        "def tokenize_de(text):\r\n",
        "    return [token.text for token in spacy_de.tokenizer(text)]\r\n",
        "\r\n",
        "def tokenize_en(text):\r\n",
        "    return [token.text for token in spacy_en.tokenizer(text)]\r\n"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7yU06wcHw5_"
      },
      "source": [
        "# torchtext.data에는 필드(Field)라는 도구를 제공하는데, 필드를 통해 앞으로 어떤 전처리를 할 것인지를 정의한다.\r\n",
        "# 번역 목표 : 소스(SRC) - 독일어, 타겟(TRG) - 영어\r\n",
        "\r\n",
        "from torchtext.data import Field, BucketIterator\r\n",
        "\r\n",
        "SRC = Field(tokenize=tokenize_de, init_token='<sos>', eos_token='<eos>', lower=True, batch_first=True)\r\n",
        "TRG = Field(tokenize=tokenize_en, init_token='<sos>', eos_token='<eos>', lower=True, batch_first=True)\r\n",
        "# 초기 입력 토큰은 문자열-시작을 알리는 (start-of-string) <SOS> 토큰으로 지정\r\n",
        "# 동일하기 마지막 토큰을 알리는 <eos> (end-of-string) 지정\r\n"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uo0i5mZxHzD_"
      },
      "source": [
        "# 파이토치의 약 3만개의 영-독 데이터 셋을 불러온다.\r\n",
        "from torchtext.datasets import Multi30k\r\n",
        "\r\n",
        "train_dataset, valid_dataset, test_dataset = Multi30k.splits(exts=(\".de\",\".en\"), fields=(SRC, TRG))\r\n"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "np64cCAfH1OV",
        "outputId": "82f3bb34-c463-45ee-a437-4979fcc3a86e"
      },
      "source": [
        "# 각 학습, 검증, 테스트 데이터로 분리된 데이터의 양 확인\r\n",
        "print(len(train_dataset.examples), len(valid_dataset.examples), len(test_dataset.examples))\r\n",
        "\r\n",
        "print(type(train_dataset.examples[30])) #인덱스 30번의 문장을 불러옴\r\n",
        "vars(train_dataset.examples[30])\r\n",
        "# vars([object])는 개체의 속성을 리턴해주는 함수\r\n",
        "# 다시말하면, torchtext.data.example은 Example 클래스 안에 소스 단어들을 속성으로 가지고 있다고 보면 됨.\r\n"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "29000 1014 1000\n",
            "<class 'torchtext.data.example.Example'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'src': ['ein',\n",
              "  'mann',\n",
              "  ',',\n",
              "  'der',\n",
              "  'mit',\n",
              "  'einer',\n",
              "  'tasse',\n",
              "  'kaffee',\n",
              "  'an',\n",
              "  'einem',\n",
              "  'urinal',\n",
              "  'steht',\n",
              "  '.'],\n",
              " 'trg': ['a',\n",
              "  'man',\n",
              "  'standing',\n",
              "  'at',\n",
              "  'a',\n",
              "  'urinal',\n",
              "  'with',\n",
              "  'a',\n",
              "  'coffee',\n",
              "  'cup',\n",
              "  '.']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkB2sz0cH51y",
        "outputId": "968a8f36-3a03-42f6-bd22-8cbce32a96c3"
      },
      "source": [
        "#field의 build_vocab() 도구를 사용하면 단어 집합(사전)을 생성할 수 있음\r\n",
        "SRC.build_vocab(train_dataset, min_freq=2)\r\n",
        "TRG.build_vocab(train_dataset, min_freq=2)\r\n",
        "# min_freq : 단어 집합에 추가 시 단어의 최소 등장 빈도 조건을 추가\r\n",
        "# 여기서는 최소 두번 이상 나온 단어만 사전으로 생성\r\n",
        "\r\n",
        "print('SRC 단어 집합의 크기 : {}'.format(len(SRC.vocab)))\r\n",
        "print('TRG 단어 집합의 크기 : {}'.format(len(TRG.vocab)))\r\n",
        "\r\n",
        "# 생성된 단어 집합 내의 단어들은 .stoi를 통해서 확인 가능\r\n",
        "# stoi --> string to i는 해당 단어가 존재하면 인덱스를 반환\r\n",
        "print(TRG.vocab.stoi['his'])\r\n",
        "# dict 형태로 저장되어 있기 때문에 키 값을 넣어 해당 단어의 인덱스 값을 찾을 수 있음\r\n",
        "\r\n",
        "test = {k:v for k,v in TRG.vocab.stoi.items()}\r\n",
        "test.get('mother')  # 496는 인덱스\r\n",
        "    "
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SRC 단어 집합의 크기 : 7855\n",
            "TRG 단어 집합의 크기 : 5893\n",
            "27\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "496"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTE4btBgH9r-"
      },
      "source": [
        "# 문장의 안의 단어의 순서를 유지하여 입력되어야 함\r\n",
        "# 이를 위해서는 각 배치에 포함되는 단어의 갯수를 맞춰주면 좋은데, BucketIterator를 사용 함\r\n",
        "# BucketIterator는 비슷한 길이를 갖는 데이터를 함께 묶는(batch) Iterator를 정의함. \r\n",
        "# 매 새로운 epoch에서 랜덤한 batch를 생성하는 과정에서 padding을 최소화하기 위해 사용\r\n",
        "\r\n",
        "# batch size란 sample데이터 중 한번에 네트워크에 넘겨주는 데이터의 수를 말한다.\r\n",
        "# 가중치와 편향을 수정하는 간격이라고도 함\r\n",
        "# 배치 사이즈는 GPU RAM에 맞는 크기로 지정하는 것이 좋음\r\n",
        "# 여기서는 논문과 동일하게 128로 하겠음\r\n",
        "\r\n",
        "import torch\r\n",
        "from torchtext.data import BucketIterator\r\n",
        "\r\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "# gpu 사용 가능할시 gpu 사용하게끔 cuda로 올려줌\r\n",
        "\r\n",
        "batch_size = 128\r\n",
        "\r\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\r\n",
        "    (train_dataset, valid_dataset, test_dataset),\r\n",
        "    batch_size=batch_size, device=device\r\n",
        ")"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDZHemqQIBGL",
        "outputId": "8e6d83a5-1dc0-4bbf-cd7e-8badf98a7466"
      },
      "source": [
        "for i, batch in enumerate(train_iterator):\r\n",
        "    src = batch.src\r\n",
        "    trg = batch.trg\r\n",
        "\r\n",
        "    print('첫번째 배치 크기 : ', {src.shape})\r\n",
        "\r\n",
        "    # 배치 1번에 포함된 문장 정보 출력\r\n",
        "    for i in range(src.shape[1]):\r\n",
        "        print(f\"인덱스 {i}: {src[0][i].item()}\") # 여기에서는 [Seq_num, Seq_len]\r\n",
        "    \r\n",
        "    # 첫번째 배치만 출력되고 중단\r\n",
        "    break\r\n",
        "\r\n",
        "    # 2 는 <sos>, 3은 <eos>, 1은 패딩 토큰을 의미\r\n",
        "    # 다시말하면 인덱스 14의 3 <eos> 뒤에는 모두 패딩 토큰으로 이 문장의 길이는 13이다. "
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "첫번째 배치 크기 :  {torch.Size([128, 29])}\n",
            "인덱스 0: 2\n",
            "인덱스 1: 8\n",
            "인덱스 2: 5792\n",
            "인덱스 3: 402\n",
            "인덱스 4: 12\n",
            "인덱스 5: 14\n",
            "인덱스 6: 1420\n",
            "인덱스 7: 27\n",
            "인덱스 8: 15\n",
            "인덱스 9: 423\n",
            "인덱스 10: 12\n",
            "인덱스 11: 4\n",
            "인덱스 12: 3\n",
            "인덱스 13: 1\n",
            "인덱스 14: 1\n",
            "인덱스 15: 1\n",
            "인덱스 16: 1\n",
            "인덱스 17: 1\n",
            "인덱스 18: 1\n",
            "인덱스 19: 1\n",
            "인덱스 20: 1\n",
            "인덱스 21: 1\n",
            "인덱스 22: 1\n",
            "인덱스 23: 1\n",
            "인덱스 24: 1\n",
            "인덱스 25: 1\n",
            "인덱스 26: 1\n",
            "인덱스 27: 1\n",
            "인덱스 28: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBoUnA_6IGBO"
      },
      "source": [
        "## 하이퍼 파라미터 설정 및 모델 초기화\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFffo2sIID1H"
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab) # 소스 언어에 포함되어 있는 언어의 개수\r\n",
        "OUTPUT_DIM = len(TRG.vocab)\r\n",
        "HIDDEN_DIM = 256\r\n",
        "# 논문보다 적은 수의 레이어를 사용함 \r\n",
        "# ==인코더/디코더 layer===================================\r\n",
        "ENC_LAYERS = 3\r\n",
        "DEC_LAYERS = 3\r\n",
        "# ===인코더/디코더 헤드==================================\r\n",
        "ENC_HEADS = 8\r\n",
        "DEC_HEADS = 8\r\n",
        "# ==인코더/디코더 feedforward 차원===================================\r\n",
        "ENC_PF_DIM = 512\r\n",
        "DEC_PF_DIM = 512\r\n",
        "# ==인코더/디코더 드롭아웃 비율===================================\r\n",
        "ENC_DROPOUT = 0.1\r\n",
        "DEC_DROPOUT = 0.1"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNR6VjIHIJMl"
      },
      "source": [
        "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\r\n",
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\r\n",
        "# 생성된 단어 집합 내의 단어들은 .stoi를 통해서 확인 가능\r\n",
        "# stoi는 문자를 정수로 바꿔주는 함수 \r\n",
        "\r\n",
        "# 인코더(encoder)와 디코더(decoder) 객체 선언\r\n",
        "enc = Encoder(INPUT_DIM, HIDDEN_DIM, ENC_LAYERS, ENC_HEADS, ENC_PF_DIM, ENC_DROPOUT, device)\r\n",
        "dec = Decoder(OUTPUT_DIM, HIDDEN_DIM, DEC_LAYERS, DEC_HEADS, DEC_PF_DIM, DEC_DROPOUT, device)\r\n",
        "\r\n",
        "# 모델 트랜스포머로 설정 \r\n",
        "model = Transformer(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Asm_VvBbsKtT"
      },
      "source": [
        "- 모델 가중치 파라미터 초기화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6Q4V1L8Nu5R",
        "outputId": "36fa5852-ccd8-4eba-df82-b6d8764e21da"
      },
      "source": [
        "# 모델의 파라미터 수를 확인한다.\r\n",
        "\r\n",
        "def count_parametrs(model):\r\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad) \r\n",
        "    # numel()은 input텐서의 총 요소 수를 반환한다. \r\n",
        "    # 텐서를 생성하고 requires_grad 속성을 True 로 설정하면, 그 tensor에서 이뤄진 모든 연산들을 추적(track)하기 시작한다. \r\n",
        "print(f'The model has {count_parametrs(model):,} trainable parameters')"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 9,038,853 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yvl1vsjPuCQ6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91d89353-9378-40b0-c44f-508dd12dd2bc"
      },
      "source": [
        "def initialize_weights(m):\r\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\r\n",
        "        nn.init.xavier_uniform_(m.weight.data)\r\n",
        "    # hasattr(object, name)는 object의 속성(attribute) 존재를 확인\r\n",
        "    # 만약 argument로 넘겨준 object 에 name 의 속성이 존재하면 True, 아니면 False를 반환\r\n",
        "    # m이 weight 속성을 가지고 있고 그 weight의 차원이 1보다 큰 경우,\r\n",
        "    # 균일 분포(uniform distribution)로 가중치를 초기화 한다. \r\n",
        "\r\n",
        "model.apply(initialize_weights)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (encoder): Encoder(\n",
              "    (tok_embedding): Embedding(7855, 256)\n",
              "    (pos_embedding): Embedding(100, 256)\n",
              "    (layers): ModuleList(\n",
              "      (0): EncoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): EncoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (2): EncoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (tok_embedding): Embedding(5893, 256)\n",
              "    (pos_embedding): Embedding(100, 256)\n",
              "    (layers): ModuleList(\n",
              "      (0): DecoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (encoder_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): DecoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (encoder_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (2): DecoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (encoder_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (fc_out): Linear(in_features=256, out_features=5893, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fchH3Y60K4G"
      },
      "source": [
        "- 학습 및 평가 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6wJWWTf0Kit"
      },
      "source": [
        "import torch.optim as optim\r\n",
        "\r\n",
        "# Adam optimizer로 학습 최적화\r\n",
        "LEARNING_RATE = 0.0005\r\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\r\n",
        "\r\n",
        "criterion = nn.CrossEntropyLoss(ignore_index= TRG_PAD_IDX)\r\n",
        "# 패딩(padding)에 대해서는 값 무시\r\n",
        "# orch.nn.CrossEntropyLoss는 nn.LogSoftmax와 nn.NLLLoss의 연산의 조합\r\n",
        "# nn.LogSoftmax는 신경망 말단의 결과 값들을 확률개념으로 해석하기 위한 Softmax 함수의 결과에 log 값을 취한 연산이고, \r\n",
        "# nn.NLLLoss는 cross-entropy 손실을 구하는 함수이다.\r\n",
        "# 만일 nn.NLLLoss만 쓴 경우에는 모델 마지막 레이어에 Softmax를 사용하게 된다.\r\n",
        "# 다시말하면, CrossEntropyLoss는 SoftMax를 적용하고 손실 값을 구하는 함수있다. "
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIAukc6LvtCC"
      },
      "source": [
        "# 모델 학습(train) 함수\r\n",
        "\r\n",
        "def train(model, iterator, optimizer, criterion, clip):\r\n",
        "    model.train() # 학습 모드\r\n",
        "    epoch_loss = 0\r\n",
        "\r\n",
        "    for i, batch in enumerate(iterator):\r\n",
        "        src = batch.src\r\n",
        "        trg = batch.trg\r\n",
        "\r\n",
        "        optimizer.zero_grad()\r\n",
        "\r\n",
        "        output, _ = model(src, trg[:,:-1])\r\n",
        "        # 출력 단어의 마지막 인덱스(<eos>)는 제외\r\n",
        "        # 입력을 할 때는 <sos>부터 시작하도록 처리\r\n",
        "        # 트랜스포머 모델은 output과 attention을 리턴함\r\n",
        "        # output: [배치 크기, trg_len - 1, output_dim]\r\n",
        "        # trg: [배치 크기, trg_len]\r\n",
        "\r\n",
        "        output_dim = output.shape[-1]\r\n",
        "\r\n",
        "        output = output.contiguous().view(-1, output_dim)\r\n",
        "        # contiguous()로 새로운 메모리 공간에 데이터를 복사하여 주소값 연속성을 가변적이게 만들어 주고,\r\n",
        "        # view() 로 텐서의 모양을 조절한다.\r\n",
        "        \r\n",
        "        trg = trg[:,1:].contiguous().view(-1)\r\n",
        "        # 출력 단어의 인덱스 0(<sos>)은 제외\r\n",
        "\r\n",
        "        # output: [배치 크기 * trg_len - 1, output_dim]\r\n",
        "        # trg: [배치 크기 * trg len - 1]\r\n",
        "\r\n",
        "        loss = criterion(output, trg)\r\n",
        "        # 모델의 출력 결과와 타겟 문장을 비교하여 손실 계산\r\n",
        "        loss.backward() # 기울기(gradient) 계산\r\n",
        "        # forward 함수는 입력 Tensor로부터 출력 Tensor를 계산합니다. backward 함수는 어떤 스칼라 값에 대한 출력 Tensor의 변화도를 전달받고, 동일한 스칼라 값에 대한 입력 Tensor의 변화도를 계산한다.\r\n",
        "        # 역전파 단계: 모델의 학습 가능한 모든 매개변수에 대해 손실의 변화도를 계산한다.\r\n",
        "        # 내부적으로 각 Module의 매개변수는 requires_grad=True 일 때, \r\n",
        "        # Tensor 내에 저장되므로, 이 호출은 모든 모델의 모든 학습 가능한 매개변수의 변화도를 계산하게 된다.\r\n",
        "\r\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\r\n",
        "        # 매 time-step마다 파라미터에 기울기가 더해지므로, 출력의 길이에 따라 기울기의 크기가 달라진다. \r\n",
        "        # 즉, 길이가 길수록 자칫 기울기가 너무 커질 수 있으므로, 학습률을 조절하여 경사하강법의 업데이트 속도를 조절해야 한다.\r\n",
        "        # 너무 큰 학습률을 사용하면 (gradient의 크기인 norm이 너무 큰 경우) \r\n",
        "        # 경사하강법에서 한 번의 업데이트 스텝의 크기가 너무 커져, 자칫 잘못된 방향으로 학습 및 발산해버릴 수 있기 때문이다.\r\n",
        "        # 이때 그래디언트 클리핑gradient clipping이 도움이 된다. \r\n",
        "        # clip_grad_norm_()는 모든 기울기를 함께 스케일(scale) 하는 함수이다. \r\n",
        "\r\n",
        "        optimizer.step()\r\n",
        "        # Optimizer의 step 함수를 호출하면 매개변수가 갱신된다.\r\n",
        "\r\n",
        "        epoch_loss += loss.item()\r\n",
        "        # loss.item()은 loss의 스칼라 값이다.\r\n",
        "        # 전체 손실 값 계산\r\n",
        "\r\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBvNFxxP6ZY9"
      },
      "source": [
        "# 모델 평가(evaluate) 함수\r\n",
        "\r\n",
        "def evaluate(model, iterator, criterion):\r\n",
        "    model.eval() # 평가모드\r\n",
        "    epoch_loss = 0\r\n",
        "\r\n",
        "    with torch.no_grad():\r\n",
        "        for i, batch in enumerate(iterator):\r\n",
        "            src = batch.src\r\n",
        "            trg = batch.trg\r\n",
        "\r\n",
        "            output, _ = model(src, trg[:,:-1])\r\n",
        "            output_dim = output.shape[-1]\r\n",
        "            output = output.contiguous().view(-1, output_dim)\r\n",
        "            trg = trg[:,1:].contiguous().view(-1)\r\n",
        "\r\n",
        "            loss = criterion(output, trg)\r\n",
        "            epoch_loss += loss.item()\r\n",
        "\r\n",
        "    return epoch_loss / len(iterator)    \r\n",
        "        "
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhGROlMgr46T"
      },
      "source": [
        "- 학습 및 검증 진행 \r\n",
        "    * 학습 횟수(epoch) : 10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXLWSC7Or0bN"
      },
      "source": [
        "# 학습 경과 시간 확인 함수\r\n",
        "\r\n",
        "import math\r\n",
        "import time\r\n",
        "\r\n",
        "def epoch_time(start_time, end_time):\r\n",
        "    elapsed_time = end_time - start_time\r\n",
        "    elapsed_mins = int(elapsed_time/60)\r\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins*60))\r\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcxBkS2zsyBw",
        "outputId": "c5585c4e-b3c3-465b-9187-29512238fc37"
      },
      "source": [
        "import time\r\n",
        "import math\r\n",
        "import random\r\n",
        "\r\n",
        "N_EPOCHS = 10 # 10 에포크 학습\r\n",
        "CLIP = 1\r\n",
        "best_valid_loss = float('inf')\r\n",
        "\r\n",
        "for epoch in range(N_EPOCHS):\r\n",
        "    start_time = time.time() #시작 시작 기록\r\n",
        "\r\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\r\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\r\n",
        "\r\n",
        "    end_time = time.time() # 종료 시간 기록\r\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n",
        "\r\n",
        "    if valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = valid_loss  # 학습하면서 가장 작은 손실 평가를 갱신함\r\n",
        "        torch.save(model.state_dict(), 'transformer_german_to_english.pt')\r\n",
        "        # state_dict는 모델 parameters를 Tensor로 매핑한 Python dict 객체\r\n",
        "\r\n",
        "    print(f'Epoch: {epoch + 1:02} | Time: {epoch_mins}m {epoch_secs}s')\r\n",
        "                # 정수 2자리로 표현\r\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):.3f}')\r\n",
        "           # \\t는 탭 들여쓰기             # 실수 소수점 3자리까지 표현\r\n",
        "        #    Math.exp()함수는 x를 인수로 하는 e^x 값을 반환, 여기서는 e는 Euler 의 상수 e (약 2.71828), e^train_loss의 형태\r\n",
        "    print(f'\\tValidation Loss: {valid_loss:.3f} | Validation PPL: {math.exp(valid_loss):.3f}')\r\n",
        "    # 펄플렉서티(perplexity, PPL)는 언어 모델을 평가하기 위한 내부 평가(Intrinsic evaluation) 지표이다. '낮을수록' 언어 모델의 성능이 좋다는 것을 의미한다.\r\n",
        "    # 언어 모델의 PPL이 10이 나왔다면, 해당 언어 모델은 테스트 데이터에 대해서 다음 단어를 예측하는 모든 시점(time-step)마다 \r\n",
        "    # 평균적으로 10개의 단어를 가지고 어떤 것이 정답인지 고민하고 있다고 볼 수 있다."
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 18s\n",
            "\tTrain Loss: 2.831 | Train PPL: 16.958\n",
            "\tValidation Loss: 2.324 | Validation PPL: 10.217\n",
            "Epoch: 02 | Time: 0m 19s\n",
            "\tTrain Loss: 2.253 | Train PPL: 9.514\n",
            "\tValidation Loss: 2.001 | Validation PPL: 7.399\n",
            "Epoch: 03 | Time: 0m 19s\n",
            "\tTrain Loss: 1.895 | Train PPL: 6.654\n",
            "\tValidation Loss: 1.805 | Validation PPL: 6.080\n",
            "Epoch: 04 | Time: 0m 19s\n",
            "\tTrain Loss: 1.646 | Train PPL: 5.188\n",
            "\tValidation Loss: 1.715 | Validation PPL: 5.555\n",
            "Epoch: 05 | Time: 0m 19s\n",
            "\tTrain Loss: 1.458 | Train PPL: 4.297\n",
            "\tValidation Loss: 1.661 | Validation PPL: 5.264\n",
            "Epoch: 06 | Time: 0m 19s\n",
            "\tTrain Loss: 1.306 | Train PPL: 3.691\n",
            "\tValidation Loss: 1.639 | Validation PPL: 5.149\n",
            "Epoch: 07 | Time: 0m 19s\n",
            "\tTrain Loss: 1.176 | Train PPL: 3.241\n",
            "\tValidation Loss: 1.628 | Validation PPL: 5.092\n",
            "Epoch: 08 | Time: 0m 19s\n",
            "\tTrain Loss: 1.067 | Train PPL: 2.906\n",
            "\tValidation Loss: 1.631 | Validation PPL: 5.111\n",
            "Epoch: 09 | Time: 0m 19s\n",
            "\tTrain Loss: 0.972 | Train PPL: 2.642\n",
            "\tValidation Loss: 1.656 | Validation PPL: 5.241\n",
            "Epoch: 10 | Time: 0m 19s\n",
            "\tTrain Loss: 0.886 | Train PPL: 2.427\n",
            "\tValidation Loss: 1.670 | Validation PPL: 5.313\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-t5pHFDav82d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "b7098e5d-3ff2-4118-c39b-ef311f01ac7f"
      },
      "source": [
        "# 학습된 모델 저장\r\n",
        "from google.colab import files\r\n",
        "\r\n",
        "files.download('transformer_german_to_english.pt')"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_80a30a57-b079-4886-b5eb-a76c59737ce5\", \"transformer_german_to_english.pt\", 36209043)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmNRt4v2pg04"
      },
      "source": [
        "- 모델 테스트(testing) 진행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDfko5vI2bpt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c517042c-8ca5-4e9f-eaf6-cd825c559ec0"
      },
      "source": [
        "model.load_state_dict(torch.load('transformer_german_to_english.pt'))\r\n",
        "\r\n",
        "test_loss = evaluate(model, test_iterator, criterion)\r\n",
        "\r\n",
        "print(f'Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):.3f}')"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 1.664 | Test PPL: 5.278\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mP7hD7L8pc80"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}